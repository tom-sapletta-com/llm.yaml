# COVAL v2.0 - Intelligent Code Repair System

![coval-logo.svg](coval-logo.svg) 

![COVAL](https://img.shields.io/badge/COVAL-v2.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.11+-green.svg)
![Ollama](https://img.shields.io/badge/Ollama-Required-orange.svg)
![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen.svg)

## ğŸ‰ **Nowe Funkcje v2.0**

### âœ… **6 Modeli LLM z Auto-pobieraniem**
- **qwen2.5-coder:7b** - Najlepszy do napraw JSON i debugowania (95% dynamicznej zdolnoÅ›ci)
- **deepseek-r1:7b** - Nowy model z reasoning capabilities (88% zdolnoÅ›ci) 
- **codellama:13b** - DuÅ¼y kontekst 32k tokenÃ³w do zÅ‚oÅ¼onych napraw (75% zdolnoÅ›ci)
- **deepseek-coder:6.7b** - Specjalizowany w analizie kodu (80% zdolnoÅ›ci)
- **granite-code:8b** - Stabilny model enterprise (70% zdolnoÅ›ci)
- **mistral:7b** - Uniwersalny fallback model (60% zdolnoÅ›ci)

### ğŸ§  **Dynamiczne Obliczanie ZdolnoÅ›ci Modelu**
System nie uÅ¼ywa juÅ¼ statycznych wartoÅ›ci 85% - teraz kalkuluje zdolnoÅ›Ä‡ modelu dynamicznie:
- **Token bonus**: +0.01% za kaÅ¼dy token ponad 8192 
- **Temperature penalty**: -20% * temperatura (optymalna 0.2)
- **Context bonus**: +0.01% za kaÅ¼dy token kontekstu ponad 8192
- **Historical bonus**: +10% * wskaÅºnik historycznego sukcesu
- **Rezultat**: Model qwen osiÄ…ga teraz 95% vs poprzednie 85%!

### ğŸ“Š **Adaptacyjna Ocena z Uczeniem SiÄ™**
- **8 kategorii problemÃ³w**: import_error, syntax_error, docker_error, type_error, runtime_error, dependency_error, config_error, other
- **Historyczne tracking**: `repairs/repair_history.json` Å›ledzi sukces/poraÅ¼kÄ™ per kategoria
- **Decay factor 0.9**: Nowsze naprawy majÄ… wiÄ™kszÄ… wagÄ™ w ocenie zdolnoÅ›ci
- **Intelligent triage**: Lepsza analiza decyzyjna repair vs rebuild

### âš™ï¸ **Konfiguracja YAML (`llm.config.yaml`)**
KaÅ¼dy model ma optymalne predefiniowane ustawienia:
```yaml
models:
  qwen2.5-coder:
    max_tokens: 16384      # ZwiÄ™kszone z 8192
    temperature: 0.2       # Optymalna dla napraw
    context_window: 32768  # DuÅ¼y kontekst
    base_capability: 0.85  # Baza dla dynamicznej kalkulacji
```

### ğŸ”„ **Automatyczne Pobieranie Modeli**
System automatycznie sprawdza dostÄ™pnoÅ›Ä‡ modelu via `ollama list` i pobiera brakujÄ…ce:
```bash
ğŸ” Sprawdzam dostÄ™pnoÅ›Ä‡ modelu: deepseek-r1:7b
ğŸ“¥ Pobieram model: deepseek-r1:7b
âœ… PomyÅ›lnie pobrano model: deepseek-r1:7b
```

## Kluczowe funkcjonalnoÅ›ci:

### 1. **Model Decyzyjny Repair vs Rebuild**
- Implementuje matematyczny model kosztu naprawy: `C_fix = Î³D * (1/S) * (1/K) * (1 + Î»(1-T))`
- Oblicza prawdopodobieÅ„stwo sukcesu uÅ¼ywajÄ…c **funkcji logitowej**
- Automatycznie decyduje czy naprawiaÄ‡ czy przebudowaÄ‡

### 2. **Workflow Naprawy (MRE â†’ Test â†’ Patch â†’ Walidacja)**
- **Triage**: Analiza problemu i zbieranie metryk
- **MRE**: Tworzenie Minimal Reproducible Example
- **Generowanie**: UÅ¼ywa LLM do tworzenia poprawek
- **Walidacja**: Automatyczne testy w Docker
- **Integracja**: Finalizacja i raportowanie

### 3. **Metryki i Analiza**
- DÅ‚ug techniczny (zÅ‚oÅ¼onoÅ›Ä‡, duplikacja, brak dokumentacji)
- Pokrycie testami
- DostÄ™pny kontekst (stacktrace, testy, dokumentacja)
- ZdolnoÅ›ci modelu LLM

### 4. **Struktura FolderÃ³w**
```
/repairs/
  /repair-{ticket-id}/
    /mre/           # Minimal Reproducible Example
    /proposals/     # Propozycje napraw
    /validation/    # Wyniki walidacji
    decision.md     # Decyzja repair vs rebuild
    repair_report.md # Raport koÅ„cowy
```

### 5. **UÅ¼ycie CLI v2.0**

#### **DostÄ™pne Modele:**
```bash
--model qwen         # qwen2.5-coder:7b (domyÅ›lny, 95% zdolnoÅ›ci)
--model deepseek     # deepseek-coder:6.7b (80% zdolnoÅ›ci)
--model codellama13b # codellama:13b (75% zdolnoÅ›ci, duÅ¼y kontekst)
--model deepseek-r1  # deepseek-r1:7b (88% zdolnoÅ›ci, reasoning)
--model granite      # granite-code:8b (70% zdolnoÅ›ci, enterprise)
--model mistral      # mistral:7b (60% zdolnoÅ›ci, fallback)
```

#### **PrzykÅ‚ady UÅ¼ycia:**

```bash
# Podstawowa naprawa z najlepszym modelem
python3 repair.py --error error.log --source ./src

# Analiza z pokazaniem dynamicznych zdolnoÅ›ci modelu
python3 repair.py --analyze --source ./src --error error.txt --model qwen

# UÅ¼yj zaawansowanego modelu z reasoning do zÅ‚oÅ¼onych bÅ‚Ä™dÃ³w
python3 repair.py --error complex_bug.log --source ./app --model deepseek-r1

# DuÅ¼y kontekst dla zÅ‚oÅ¼onych projektÃ³w
python3 repair.py --error error.txt --source ./large_project --model codellama13b

# Z testem i verbose logging
python3 repair.py --error stacktrace.txt --source ./project \
  --test tests/test_bug.py --model qwen --verbose

# Enterprise-grade model dla produkcji
python3 repair.py --error prod_error.log --source ./enterprise_app \
  --model granite --ticket PROD-1234

# Tylko analiza z porÃ³wnaniem modeli
python3 repair.py --analyze --source ./src --error bug.txt --model deepseek-r1
python3 repair.py --analyze --source ./src --error bug.txt --model codellama13b
```

#### **PrzykÅ‚ad WyjÅ›cia z v2.0:**
```bash
$ python3 repair.py --analyze --source ./app --error error.log --model qwen

ğŸ” Sprawdzam dostÄ™pnoÅ›Ä‡ modelu: qwen2.5-coder:7b
âœ… Model qwen2.5-coder:7b jest dostÄ™pny
ğŸ¤– UÅ¼yto modelu: qwen2.5-coder:7b
âš™ï¸  Konfiguracja: 16384 tokenÃ³w, temp: 0.2
ğŸ“Š Tryb analizy (bez naprawy)

============================================================
ğŸ“Š ANALIZA DECYZYJNA v2.0
============================================================
Rekomendacja: REBUILD
PrawdopodobieÅ„stwo sukcesu: 65.63%
Koszt naprawy: 1052.63 â†“ (niÅ¼szy dziÄ™ki lepszej zdolnoÅ›ci!)
Koszt przebudowy: 10.18

Metryki:
  - DÅ‚ug techniczny: 2.00
  - Pokrycie testami: 0.00%
  - DostÄ™pny kontekst: 0.00%
  - ZdolnoÅ›ci modelu: 95.00% â†‘ (dynamiczne!)
  - Historyczna skutecznoÅ›Ä‡: 0.00% (nowy system)
  - Kategoria problemu: import_error
  - Model uÅ¼yty: qwen2.5-coder:7b
  - Parametry: 16384 tokenÃ³w, temp: 0.2
============================================================
```

### 6. **Inteligentne Funkcje v2.0**

- **Automatyczne pobieranie modeli** - System sprawdza `ollama list` i pobiera brakujÄ…ce modele
- **Dynamiczne obliczanie zdolnoÅ›ci** - UwzglÄ™dnia tokeny, temperaturÄ™, kontekst i historiÄ™
- **Adaptacyjne uczenie siÄ™** - 8 kategorii problemÃ³w z historical tracking
- **Automatyczne wykrywanie jÄ™zyka/frameworka** - dostosowuje Dockerfile i proces walidacji
- **Iteracyjne poprawki** - do 5 prÃ³b z rÃ³Å¼nymi podejÅ›ciami i konfiguracjÄ… z YAML
- **Parsowanie bÅ‚Ä™dÃ³w** - wyciÄ…ga pliki wymienione w stacktrace
- **Generowanie promptÃ³w** - rÃ³Å¼ne szablony dla pierwszej i kolejnych prÃ³b
- **Walidacja w kontenerach** - izolowane Å›rodowisko testowe
- **Konfigurowalne parametry** - Wszystkie ustawienia modeli w `llm.config.yaml`

## ğŸ“¦ **Instalacja i Wymagania v2.0**

### **Wymagania Systemowe:**
```bash
# Podstawowe wymagania
Python 3.11+
Docker & Docker Compose
Ollama (automatyczne pobieranie modeli)

# Python dependencies
pip install pyyaml requests docker subprocess32
```

### **Instalacja Ollama:**
```bash
# Linux/macOS
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# Pobierz z https://ollama.com/download

# Uruchom ollama
ollama serve
```

### **Konfiguracja COVAL:**
```bash
# Klonuj repozytorium
git clone https://github.com/twÃ³j-repo/ymll.git
cd ymll/coval

# System automatycznie pobierze modele przy pierwszym uÅ¼yciu
python3 repair.py --analyze --source ./test --error ./test.log --model qwen

# SprawdÅº dostÄ™pne modele
ollama list
```

## ğŸ”§ **Troubleshooting**

### **Problemy z Ollama:**
```bash
# Ollama nie jest zainstalowane
âŒ Ollama nie jest zainstalowane lub nie jest w PATH
âœ… RozwiÄ…zanie: curl -fsSL https://ollama.com/install.sh | sh

# Model nie moÅ¼e byÄ‡ pobrany
âŒ BÅ‚Ä…d pobierania modelu: connection refused
âœ… RozwiÄ…zanie: Uruchom ollama serve w osobnym terminalu

# Timeout pobierania
â±ï¸ Timeout przy pobieraniu modelu: deepseek-r1:7b
âœ… RozwiÄ…zanie: ZwiÄ™ksz timeout lub pobierz rÄ™cznie: ollama pull deepseek-r1:7b
```

### **Problemy z KonfiguracjÄ…:**
```bash
# Brak llm.config.yaml
âŒ Nie moÅ¼na zaÅ‚adowaÄ‡ konfiguracji
âœ… RozwiÄ…zanie: Skopiuj llm.config.yaml z repozytorium

# NieprawidÅ‚owa konfiguracja YAML
âŒ yaml.parser.ParserError
âœ… RozwiÄ…zanie: SprawdÅº skÅ‚adniÄ™ YAML online (yamllint.com)

# Brak uprawnieÅ„ do zapisu repair_history.json
âŒ Permission denied: repairs/repair_history.json
âœ… RozwiÄ…zanie: mkdir -p repairs && chmod 755 repairs
```

### **Problemy z Modelami:**
```bash
# Model nie odpowiada
âŒ Model timeout po 60s
âœ… RozwiÄ…zanie: UÅ¼yj mniejszego modelu (--model mistral) lub zwiÄ™ksz timeout

# NiewystarczajÄ…ca pamiÄ™Ä‡
âŒ CUDA out of memory
âœ… RozwiÄ…zanie: UÅ¼yj CPU: CUDA_VISIBLE_DEVICES="" python3 repair.py

# Model daje zÅ‚e wyniki
âŒ Repair failed repeatedly
âœ… RozwiÄ…zanie: SprÃ³buj innego modelu z wiÄ™kszymi zdolnoÅ›ciami (--model deepseek-r1)
```

## ğŸ“ˆ **PorÃ³wnanie WydajnoÅ›ci v1.0 vs v2.0**

### **Analiza tego samego problemu (import_error):**

| Metryka | v1.0 (Static) | v2.0 (Dynamic) | Poprawa |
|---------|---------------|----------------|---------|
| **ZdolnoÅ›Ä‡ modelu** | 85.00% (static) | 95.00% (dynamic) | **+10%** â†‘ |
| **PrawdopodobieÅ„stwo sukcesu** | 64.04% | 65.63% | **+1.59%** â†‘ |
| **Koszt naprawy** | 1176.47 | 1052.63 | **-123.84** â†“ |
| **DostÄ™pne modele** | 1 (qwen) | 6 modeli | **+500%** â†‘ |
| **KonfigurowalnoÅ›Ä‡** | Brak | YAML config | **Nowa funkcja** |
| **Uczenie siÄ™** | Brak | Historical tracking | **Nowa funkcja** |
| **Auto-pobieranie** | RÄ™czne | Automatyczne | **Nowa funkcja** |

### **Kluczowe Ulepszenia:**

#### ğŸ¯ **Lepsza Analiza Decyzyjna**
- **Dynamiczne zdolnoÅ›ci**: System uwzglÄ™dnia rzeczywiste parametry modelu
- **Kategoryzacja problemÃ³w**: 8 typÃ³w bÅ‚Ä™dÃ³w vs generyczne podejÅ›cie  
- **Historyczne uczenie**: System poprawia siÄ™ z kaÅ¼dÄ… naprawÄ…
- **NiÅ¼sze koszty**: Lepsze zdolnoÅ›ci = mniejszy koszt naprawy

#### ğŸ¤– **Szerszy WybÃ³r Modeli**
```
v1.0: Tylko qwen2.5-coder (85% static capability)
v2.0: 6 modeli z dynamicznymi zdolnoÅ›ciami:
â”œâ”€ qwen2.5-coder:7b    â†’ 95% (najlepszy do JSON/debugowania)
â”œâ”€ deepseek-r1:7b      â†’ 88% (reasoning capabilities)  
â”œâ”€ deepseek-coder:6.7b â†’ 80% (kod specjalizowany)
â”œâ”€ codellama:13b       â†’ 75% (duÅ¼y kontekst 32k)
â”œâ”€ granite-code:8b     â†’ 70% (enterprise grade)
â””â”€ mistral:7b          â†’ 60% (fallback uniwersalny)
```

#### âš™ï¸ **Åatwiejsza Konfiguracja**
```
v1.0: Twarde kodowanie parametrÃ³w w kodzie
v2.0: Centralna konfiguracja YAML:
â”œâ”€ Optymalne ustawienia per model
â”œâ”€ Konfigurowalne timeouty  
â”œâ”€ Adaptacyjne parametry uczenia
â””â”€ Åatwa customizacja bez edycji kodu
```

## âš™ï¸ **SzczegÃ³Å‚owa Konfiguracja `llm.config.yaml`**

### **PeÅ‚na Struktura Pliku:**
```yaml
# Globalne ustawienia systemu
global:
  timeout: 60
  max_iterations: 5
  adaptive_evaluation:
    enabled: true
    history_weight: 0.3
    decay_factor: 0.9
    min_samples: 5
  capability_calculation:
    token_bonus_multiplier: 0.0001    # +0.01% za token ponad 8192
    temperature_penalty: 0.2          # -20% * temperatura
    context_bonus_multiplier: 0.0001  # +0.01% za token kontekstu ponad 8192
    max_capability: 0.95              # Maksymalna zdolnoÅ›Ä‡ (95%)

# Konfiguracje poszczegÃ³lnych modeli
models:
  # Model domyÅ›lny - najlepszy do napraw JSON
  qwen2.5-coder:
    model_name: "qwen2.5-coder:7b"
    max_tokens: 16384           # 2x wiÄ™cej niÅ¼ standard
    temperature: 0.2            # Optymalna dla napraw
    retry_attempts: 3
    base_capability: 0.85       # Baza dla dynamicznej kalkulacji
    context_window: 32768       # DuÅ¼y kontekst
    specialization: ["json", "debugging", "python", "javascript"]
    
  # Nowy model z reasoning - do zÅ‚oÅ¼onych problemÃ³w
  deepseek-r1:
    model_name: "deepseek-r1:7b"
    max_tokens: 12288
    temperature: 0.1            # Niska dla reasoning
    retry_attempts: 4
    base_capability: 0.80
    context_window: 16384
    specialization: ["reasoning", "complex_logic", "algorithms"]
    
  # DuÅ¼y model - do wiÄ™kszych projektÃ³w
  codellama:
    model_name: "codellama:13b"
    max_tokens: 8192
    temperature: 0.3
    retry_attempts: 2
    base_capability: 0.70
    context_window: 32768       # NajwiÄ™kszy kontekst
    specialization: ["large_codebases", "refactoring", "architecture"]
```

### **Customizacja dla WÅ‚asnych Potrzeb:**

#### **ZwiÄ™ksz ZdolnoÅ›ci Modelu:**
```yaml
models:
  custom-qwen:
    base_capability: 0.90      # â†‘ WyÅ¼sza baza
    max_tokens: 32768          # â†‘ WiÄ™cej tokenÃ³w = bonus
    temperature: 0.1           # â†“ NiÅ¼sza temperatura = mniej penalty
    context_window: 65536      # â†‘ WiÄ™kszy kontekst = bonus
```

#### **Dostosuj dla Åšrodowiska Produkcyjnego:**
```yaml
global:
  timeout: 120                 # â†‘ WiÄ™cej czasu dla zÅ‚oÅ¼onych napraw
  max_iterations: 10           # â†‘ WiÄ™cej prÃ³b
  adaptive_evaluation:
    history_weight: 0.5        # â†‘ WiÄ™ksza waga dla historii
    min_samples: 10            # â†‘ WiÄ™cej danych do oceny
```

#### **Optymalizacja dla SzybkoÅ›ci:**
```yaml
models:
  fast-mistral:
    max_tokens: 4096           # â†“ Mniej tokenÃ³w = szybciej
    temperature: 0.4           # â†‘ WyÅ¼sza = mniej precyzyjne ale szybsze
    retry_attempts: 1          # â†“ Mniej prÃ³b
```

### 7. **Raporty i Decyzje**

System generuje szczegÃ³Å‚owe raporty zawierajÄ…ce:
- AnalizÄ™ kosztÃ³w (repair vs rebuild)
- PrawdopodobieÅ„stwo sukcesu
- Zastosowane poprawki
- OcenÄ™ ryzyka regresji
- Rekomendacje dalszych krokÃ³w

### 8. **Wsparcie dla wielu jÄ™zykÃ³w**

Automatycznie rozpoznaje i obsÅ‚uguje:
- Python (FastAPI, Django, Flask)
- JavaScript/Node.js (Express, Next.js)
- Go (Gin, Fiber)
- Rust, Java, Ruby, PHP

Skrypt jest w peÅ‚ni zintegrowany z podejÅ›ciem YMLL i implementuje wszystkie najlepsze praktyki z REPAIR_GUIDELINES, zapewniajÄ…c efektywny i powtarzalny proces naprawiania kodu z pomocÄ… LLM.

## Funkcja logitowa

Funkcja **logitowa** to po prostu funkcja matematyczna uÅ¼ywana gÅ‚Ã³wnie w statystyce i uczeniu maszynowym do przeksztaÅ‚cania prawdopodobieÅ„stw w tzw. log-odds. Jest odwrotnoÅ›ciÄ… funkcji sigmoidalnej (logistycznej).

DokÅ‚adniej:

### Definicja

JeÅ¼eli $p$ to prawdopodobieÅ„stwo zdarzenia (0 < p < 1), funkcja logitowa jest zdefiniowana jako:

$$
\text{logit}(p) = \ln\left(\frac{p}{1-p}\right)
$$

* $p/(1-p)$ to **odds** (szansa, Å¼e zdarzenie nastÄ…pi vs Å¼e nie nastÄ…pi)
* $\ln$ to logarytm naturalny

### PrzykÅ‚ad

* JeÅ›li $p = 0.8$ (80% prawdopodobieÅ„stwa),

$$
\text{logit}(0.8) = \ln\left(\frac{0.8}{0.2}\right) = \ln(4) \approx 1.386
$$

* JeÅ›li $p = 0.5$, $\text{logit}(0.5) = \ln(1) = 0$

### Zastosowanie

* W **regresji logistycznej** logit przeksztaÅ‚ca prawdopodobieÅ„stwa w wartoÅ›Ä‡ na osi liczbowej od $-\infty$ do $+\infty$, co pozwala modelowi liniowemu prognozowaÄ‡ log-odds, a nastÄ™pnie Å‚atwo przeksztaÅ‚caÄ‡ z powrotem w prawdopodobieÅ„stwo.
* W twoim kontekÅ›cie (system naprawy kodu) funkcja logitowa moÅ¼e sÅ‚uÅ¼yÄ‡ do obliczenia **prawdopodobieÅ„stwa sukcesu naprawy** na podstawie rÃ³Å¼nych zmiennych (jak zÅ‚oÅ¼onoÅ›Ä‡, dostÄ™pnoÅ›Ä‡ testÃ³w itd.).

